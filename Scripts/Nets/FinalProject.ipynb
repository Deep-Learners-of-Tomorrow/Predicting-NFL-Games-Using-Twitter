{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7gL7IovmhCXA",
        "RyEY2R0B6cgN",
        "KwKKtG0t6hUU",
        "E0kj8_T46sdc",
        "JnlPM4rdJPXW",
        "BdwNiR5zDT6o"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEJUgLaOgcuc"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi7qJujXOb44"
      },
      "source": [
        "# the capture thing just supresses output from the pip installs\n",
        "%%capture \n",
        "%pip install pytorch_lightning\n",
        "%pip install numpy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqPMrrKxtjHV"
      },
      "source": [
        "import torch             \n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.loggers as pl_loggers\n",
        "from time import process_time \n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pytorch_lightning import seed_everything\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqyTyceWvkdY"
      },
      "source": [
        "#Function for setting seeds, should be called before each experiment\n",
        "def set_seeds():\n",
        "  torch.manual_seed(0)\n",
        "  np.random.seed(0)\n",
        "  torch.set_deterministic(True)\n",
        "  seed_everything(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lzXxkfVEV9P"
      },
      "source": [
        "# functions for plotting stuff\n",
        "\n",
        "#returns moving average of python list data series\n",
        "def moving_average(a):\n",
        "  leng = len(a)\n",
        "  m_a = []\n",
        "  sum = 0\n",
        "  for i in range(leng):\n",
        "    sum += a[i]\n",
        "    val = float(sum) / (i + 1)\n",
        "    m_a.append(val)\n",
        "  return m_a\n",
        "\n",
        "#returns exponential moving average of python list data series\n",
        "def ema(a):\n",
        "  leng = len(a)\n",
        "  ema = []\n",
        "  ema.append(a[0])\n",
        "  smoothing = float(2)\n",
        "  for i in range(1, leng):\n",
        "    v1 = a[i] * (smoothing / (i + 2))\n",
        "    v2 = ema[i - 1] * (1  - (smoothing / (i + 2)))\n",
        "    ema.append(v1 + v2)\n",
        "  return ema\n",
        "\n",
        "def plot_data(training, validation, metric, average_method = 'ema', save = [False,''],show = True):\n",
        "  if average_method == 'ema':\n",
        "    plt.plot(ema(training), 'r', label = \"Training \"+metric)\n",
        "    plt.plot(ema(validation), 'b', label = \"Validation \"+metric)\n",
        "  elif average_method == 'moving':\n",
        "    plt.plot(moving_average(training), 'r', label = \"Training \"+metric)\n",
        "    plt.plot(moving_average(validation), 'b', label = \"Validation \"+metric)\n",
        "  plt.title(f\"Training and Validation {metric} Over Steps ({save[1]})\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.xlabel('Steps')\n",
        "  plt.legend()\n",
        "  if save[0] == True:\n",
        "    plt.savefig(\"/content/drive/MyDrive/Deep Learning Final Project/Graphs/\"+str(save[1])+\".png\")\n",
        "    plt.clf()\n",
        "  if show == True:\n",
        "    plt.show()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gL7IovmhCXA"
      },
      "source": [
        "### Connecting to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pwy7Qpfm0vT",
        "outputId": "a2ef0845-3748-4b21-a490-2bce797feb00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyEY2R0B6cgN"
      },
      "source": [
        "### Our Trivial Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5FJTaY-KeGU"
      },
      "source": [
        "#We get 100% accuracy on this\n",
        "#Trivial dataset\n",
        "class Trivial(torch.utils.data.Dataset):\n",
        "  def __init__(self, train):\n",
        "    data_arr = []\n",
        "  \n",
        "    #Order of Data\n",
        "    #['Home Scoring', 'Home Pass Yds', 'Home Rush Yds', 'Home Fumbles', 'Home Ints', 'Away Scoring', 'Away Pass Yds', 'Away Rush Yds', 'Away Fumbles', 'Away Ints', 'Home Field Adv']\n",
        "\n",
        "    #Should Predict wins here\n",
        "    line1 = [100, 100, 100, 0, 0, 0, 0, 0, 10, 20, 0.5]\n",
        "    line2 = [x*2 for x in line1[:-1]]\n",
        "    line2.append(0.5)\n",
        "    line3 = [x*3 for x in line1[:-1]]\n",
        "    line3.append(0.5)\n",
        "\n",
        "    #Should predict losses here\n",
        "    line4 = [0, 0, 0, 10, 10, 100, 100, 100, 0, 0, 0.5]\n",
        "    line5 = [x*2 for x in line4[:-1]]\n",
        "    line5.append(0.5)\n",
        "    line6 = [x*3 for x in line4[:-1]]\n",
        "    line6.append(0.5)\n",
        "\n",
        "    #Win or Loss outcomes for the home team\n",
        "    targets = [1,1,1,0,0,0]\n",
        "\n",
        "    if train:\n",
        "      targets = [1,1,0,0]\n",
        "      data_arr.append(line1)\n",
        "      data_arr.append(line2)\n",
        "      data_arr.append(line4)\n",
        "      data_arr.append(line5)\n",
        "      print(data_arr)\n",
        "      \n",
        "      self.ex = torch.tensor(data_arr).to(dtype=torch.float32)\n",
        "      #Might need to make labels ints\n",
        "      self.labels = torch.FloatTensor(targets)\n",
        "      \n",
        "      self.leng = 4\n",
        "\n",
        "    else:\n",
        "      targets = [1,0]\n",
        "      data_arr.append(line3)\n",
        "      data_arr.append(line6)\n",
        "      self.ex = torch.tensor(data_arr).to(dtype=torch.float32)\n",
        "      #Might need to make labels ints\n",
        "      self.labels = torch.tensor(targets).to(dtype=torch.float32)\n",
        "      self.leng = 2\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.leng\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.ex[idx], self.labels[idx]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwKKtG0t6hUU"
      },
      "source": [
        "### Our Model And Acutal Dataset/Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeLj3GqfuFjt"
      },
      "source": [
        "class nflStatsModule(pl.LightningModule):\n",
        "  # Define the model architecture\n",
        "  def __init__(self, batch_size = 1, twitter = False):\n",
        "    super(nflStatsModule, self).__init__()\n",
        "    # if using twitter the input array has 4 extra params\n",
        "    if twitter:\n",
        "      self.layer_1 = torch.nn.Linear(15, 4)\n",
        "    else:\n",
        "      self.layer_1 = torch.nn.Linear(11, 4)\n",
        "\n",
        "    # the other layers are the same for both nets\n",
        "    self.layer_2 = torch.nn.Linear(4, 6)\n",
        "    self.layer_3 = torch.nn.Linear(6, 2)\n",
        "\n",
        "    #Set variables that measure performance\n",
        "    self.correct_predictions = []\n",
        "    self.validation_loss = []\n",
        "    self.training_loss = []\n",
        "    self.validation_accuracy = []\n",
        "    self.training_accuracy = []\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = torch.tanh(self.layer_1(x))\n",
        "      x = torch.tanh(self.layer_2(x))\n",
        "      x = F.softmax(self.layer_3(x))\n",
        "      return x\n",
        "\n",
        "  #We will stick with this optimizer\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  #Use this loss, so predictions are 0s and 1s\n",
        "  def my_loss(self, y_hat, y):\n",
        "    # print('output is')\n",
        "    # print(y_hat)\n",
        "    # print(y_hat[0])\n",
        "    # print('label is')\n",
        "    # print(y)\n",
        "    # if int(y.item()) == 1:\n",
        "    #   target_tensor = torch.tensor([0,1]).to(dtype=torch.float32)\n",
        "    # else:\n",
        "    #   target_tensor = torch.tensor([1,0]).to(dtype=torch.float32)\n",
        "\n",
        "    return torch.nn.functional.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "    x, y = train_batch  # Here x = data, y = labels\n",
        "    output = self.forward(x)\n",
        "    loss = self.my_loss(output, y)\n",
        "    self.training_loss.append(loss)\n",
        "    \n",
        "    # # Calculate the accuracy of the model on the batch of data\n",
        "    y_hat =  output.argmax(dim=1)\n",
        "    y = y.argmax(dim=1)\n",
        "    \n",
        "    \n",
        "    accuracy = y_hat.eq(y).sum().item()/len(y)\n",
        "    self.training_accuracy.append(accuracy)\n",
        "\n",
        "    # # these two lines write the accurcay and loss to TensorBoard\n",
        "    # self.logger.experiment.add_scalar(\"Accuracy/Train\", accuracy)\n",
        "    # self.logger.experiment.add_scalar(\"Loss/Train\", loss)\n",
        "\n",
        "    return {\"loss\": loss} \n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "    x, y = val_batch\n",
        "    output = self.forward(x)\n",
        "    loss = self.my_loss(output, y)\n",
        "    self.logger.experiment.add_scalar(\"Loss/Val\", loss)\n",
        "    self.validation_loss.append(loss)\n",
        "\n",
        "    y_hat =  output.argmax(dim=1)\n",
        "    y = y.argmax(dim=1)\n",
        "\n",
        "    accuracy = y_hat.eq(y).sum().item()/len(y)\n",
        "    self.validation_accuracy.append(accuracy)\n",
        "    return {\"loss\":loss}\n",
        "\n",
        "  def test_step(self, test_batch, batch_idx):\n",
        "    x, y = test_batch\n",
        "    output = self.forward(x)\n",
        "    \n",
        "    pred = torch.argmax(output[0])\n",
        "    if pred == torch.argmax(y[0]):\n",
        "      self.correct_predictions.append(1)\n",
        "    else:\n",
        "      self.correct_predictions.append(0)\n",
        "    loss = self.my_loss(output, y)\n",
        "    return {\"loss\":loss}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGJJGdSRyOaK"
      },
      "source": [
        "class nflStatsDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self,train_batch = 1, val_batch = 1, data_dir = ''):\n",
        "        super().__init__()\n",
        "        self.train_batch = train_batch\n",
        "        self.val_batch = val_batch\n",
        "        self.dataset = nflStatsDataset(data_dir)\n",
        "        \n",
        "    \n",
        "    #Split the data up\n",
        "    def setup(self, stage=None):\n",
        "        self.train, self.val, self.test = torch.utils.data.random_split(self.dataset, [145*3, 35*3 - 1, 44*3]) #[415, 128, 128])\n",
        "        \n",
        "\n",
        "    # Dataloaders are the things that handle creating batches of data and handing them\n",
        "    # to the model. You determine whether to randomize data order and the size of the batch\n",
        "    # when you declare the data loader\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train, batch_size=self.train_batch, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val, batch_size=self.val_batch, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.test, batch_size=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CtOEdMeNtIK"
      },
      "source": [
        "# HERE'S THE DATASET TO HELP YOU TEST/TRAIN IT\n",
        "class nflStatsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.df = pd.read_csv(data_dir)\n",
        "        self.length = len(self.df)\n",
        "\n",
        "        \n",
        "        #We need this, conversion from pandas to csv creates an extra column\n",
        "        try:\n",
        "          self.df = self.df.drop(columns=['Unnamed: 0'])\n",
        "        except:\n",
        "          pass\n",
        "        \n",
        "        #Predetermined Length; 256 Total Games (32*16 / 2); subtract two weeks 256 - 16*2 = 224 \n",
        "        #1D target Array\n",
        "        target_vals = self.df['Target'].values\n",
        "        new_vals = []\n",
        "        for targ in target_vals:\n",
        "          if int(targ) == 1:\n",
        "            new_targ = [0.0,1.0]\n",
        "          else:\n",
        "            new_targ = [1.0,0.0]\n",
        "          new_vals.append(new_targ)\n",
        "\n",
        "        self.label = torch.tensor(new_vals)\n",
        "\n",
        "        data_df = self.df.drop(columns = ['Target'])\n",
        "        self.data = torch.tensor(data_df.values).to(dtype=torch.float32)\n",
        "       \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.label[idx]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0kj8_T46sdc"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnlPM4rdJPXW"
      },
      "source": [
        "### Single Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNJhSJeJTWS"
      },
      "source": [
        "\n",
        "set_seeds()\n",
        "model = nflStatsModule(256, twitter = False)\n",
        "\n",
        "trainer = pl.Trainer(gpus = 1, deterministic=True, progress_bar_refresh_rate=0,max_steps=10000, max_epochs=8000) #,limit_val_batches=0.125) #Limit val batches\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Deep Learning Final Project/Data/all_years.csv\"\n",
        "data_module = nflStatsDataModule(model.batch_size, data_path)\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "\n",
        "#Plot Data\n",
        "plot_data(model.training_loss,model.validation_loss, 'Loss',save = [False, \"twitter_loss_20ksteps_\"+ str(i)+\"batch\"],show= True)\n",
        "plot_data(model.training_accuracy,model.validation_accuracy, 'Accuracy',save = [False,\"twitter_acc_20ksteps_\"+ str(i)+\"batch\"],show = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBsx0CeG8NGK"
      },
      "source": [
        "set_seeds()\n",
        "for i in [16]:\n",
        "  model = nflStatsModule(batch_size = i,twitter = True)\n",
        "\n",
        "  trainer = pl.Trainer(gpus = 1, max_steps=15000, deterministic=True, progress_bar_refresh_rate=0,max_epochs=15000) #,limit_val_batches=0.125) #Limit val batches\n",
        "\n",
        "  data_path = \"/content/drive/MyDrive/Deep Learning Final Project/Data/all_years_twitter.csv\"\n",
        "  data_module = nflStatsDataModule(model.batch_size,data_path)\n",
        "\n",
        "  trainer.fit(model, data_module)\n",
        "\n",
        "  #Plot Data\n",
        "  plot_data(model.training_loss,model.validation_loss, 'Loss', save= [False,\"twitter_loss_20ksteps_\"+ str(i)+\"batch\"], show = True)\n",
        "  plot_data(model.training_accuracy,model.validation_accuracy, 'Accuracy',save= [False,\"twitter_acc_20ksteps_\"+ str(i)+\"batch\"], show = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBesalm7nsza"
      },
      "source": [
        "print(model.validation_accuracy[-10:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rANKa7mPlfBO"
      },
      "source": [
        "# Final Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s89mydJYlcdM"
      },
      "source": [
        "set_seeds()\n",
        "\n",
        "model2 = nflStatsModule(twitter = True)\n",
        "\n",
        "trainer = pl.Trainer(gpus = 1, max_steps=1000, deterministic=True, progress_bar_refresh_rate=0) #max_epochs=8000) #,limit_val_batches=0.125) #Limit val batches\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Deep Learning Final Project/Data/all_years_twitter.csv\"\n",
        "data_module = nflStatsDataModule(train_batch=29, val_batch=7, data_dir=data_path)\n",
        "\n",
        "trainer.fit(model2, data_module)\n",
        "\n",
        "#This will come later, this is how we will track, final accuracy and how well our net performs\n",
        "trainer.test(model2, datamodule=data_module)\n",
        "\n",
        "plt.plot(ema(model2.training_loss), 'r', label = \"Training Loss\")\n",
        "plt.plot(ema(model2.validation_loss), 'b', label = \"Validation Loss\")\n",
        "plt.title('Loss over steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(ema(model2.training_accuracy), 'r', label = \"Training Accuracy\")\n",
        "plt.plot(ema(model2.validation_accuracy), 'b', label = \"Validation Accuracy\")\n",
        "plt.title('Accuracy over steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(model2.validation_accuracy[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05u_MS8I3jIt"
      },
      "source": [
        "set_seeds()\n",
        "\n",
        "model = nflStatsModule()\n",
        "\n",
        "trainer = pl.Trainer(gpus = 1, max_steps=9000, deterministic=True, progress_bar_refresh_rate=0) #max_epochs=8000) #,limit_val_batches=0.125) #Limit val batches\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Deep Learning Final Project/Data/all_years.csv\"\n",
        "data_module = nflStatsDataModule(train_batch=29, val_batch=7, data_dir=data_path)\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "#This will come later, this is how we will track, final accuracy and how well our net performs\n",
        "trainer.test(model, datamodule=data_module)\n",
        "\n",
        "plt.plot(ema(model.training_loss), 'r', label = \"Training Loss\")\n",
        "plt.plot(ema(model.validation_loss), 'b', label = \"Validation Loss\")\n",
        "plt.title('Loss over steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(ema(model.training_accuracy), 'r', label = \"Training Accuracy\")\n",
        "plt.plot(ema(model.validation_accuracy), 'b', label = \"Validation Accuracy\")\n",
        "plt.title('Accuracy over steps')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Steps')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(model.validation_accuracy[-1])\n",
        "# #Plot Data\n",
        "\n",
        "# plot_data(model2.training_loss, model2.validation_loss, 'Loss', save= [True,\"twitter_loss_1ksteps\"], show = True)\n",
        "# plot_data(model2.training_accuracy,model2.validation_accuracy, 'Accuracy',save= [True,\"twitter_acc_1ksteps\"], show = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbr3eUUOWaYZ"
      },
      "source": [
        "print(model.correct_predictions)\n",
        "accuracy = 100 * float(sum(model.correct_predictions)) / len(model.correct_predictions)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}